# ğŸ“š System RAG - Baza Wiedzy dla Wirtualnego Asystenta

## Czym jest RAG?

**RAG (Retrieval-Augmented Generation)** to system, ktÃ³ry wzbogaca odpowiedzi LLM o kontekst z bazy wiedzy. 

W naszym przypadku:
- Dokumentacja z folderu `docs/` jest przetwarzana i zapisywana jako **embedingi wektorowe**
- Gdy uÅ¼ytkownik zadaje pytanie o system (np. "jak dziaÅ‚a aplikacja?"), RAG automatycznie wyszukuje relevantne fragmenty dokumentacji
- Te fragmenty sÄ… dodawane do kontekstu LLM, dziÄ™ki czemu asystent moÅ¼e odpowiedzieÄ‡ na pytania o architekturÄ™, funkcjonalnoÅ›Ä‡, API itp.

---

## ğŸš€ Szybki Start

### 1. Instalacja zaleÅ¼noÅ›ci

```bash
cd backend
pip install sentence-transformers chromadb langchain-text-splitters
```

lub z pliku requirements:

```bash
pip install -r requriements.txt
```

### 2. Budowa bazy wiedzy

Uruchom standalone skrypt, ktÃ³ry przetworzy dokumentacjÄ™:

```bash
python build_rag_database.py
```

**Co robi ten skrypt:**
- Wczytuje wszystkie pliki `.md` z folderu `docs/`
- Dzieli je na chunki (fragmenty ~1000 znakÃ³w)
- Generuje embedingi uÅ¼ywajÄ…c modelu `sdadas/mmlw-retrieval-roberta-large` (najlepszy dla polskiego)
- Zapisuje wszystko do bazy ChromaDB w folderze `backend/assistant_ai/chroma_db/`

**Pierwszy raz moÅ¼e potrwaÄ‡ kilka minut** - model embedingowy musi siÄ™ pobraÄ‡ (~500MB).

### 3. Gotowe!

System automatycznie zaÅ‚aduje bazÄ™ wiedzy przy starcie asystenta. Nie musisz nic wiÄ™cej robiÄ‡!

---

## ğŸ”§ Jak to dziaÅ‚a?

### Architektura

```
docs/
â”œâ”€â”€ Asystent_AI_Dokumentacja.md
â”œâ”€â”€ Asystent_AI_Architektura.md
â””â”€â”€ ... inne pliki .md
         â†“
    [build_rag_database.py]
         â†“
    Chunking (podziel na fragmenty)
         â†“
    Embedding (zamieÅ„ na wektory)
         â†“
backend/assistant_ai/chroma_db/
    (baza wektorowa ChromaDB)
         â†“
    [rag_knowledge.py]
         â†“
    Semantic Search
         â†“
    [core.py - VirtualAssistant]
         â†“
    Kontekst dla LLM
```

### Komponenty

#### 1. `build_rag_database.py` - Budowa bazy (standalone)
- **Nie jest** czÄ™Å›ciÄ… gÅ‚Ã³wnego systemu
- UÅ¼ywany tylko do jednorazowego utworzenia/aktualizacji bazy
- MoÅ¼e byÄ‡ uruchamiany wielokrotnie (nadpisuje starÄ… bazÄ™)

#### 2. `assistant_ai/rag_knowledge.py` - ModuÅ‚ RAG (runtime)
- Klasa `RAGKnowledgeBase` - wyszukiwanie w bazie
- Metoda `search()` - semantyczne wyszukiwanie fragmentÃ³w
- Metoda `get_context_for_query()` - zwraca sformatowany kontekst

#### 3. `assistant_ai/core.py` - Integracja
- VirtualAssistant inicjalizuje `RAGKnowledgeBase`
- Metoda `_check_and_get_rag_context()` sprawdza czy pytanie dotyczy dokumentacji
- JeÅ›li tak, automatycznie dodaje kontekst z bazy do prompta

#### 4. `prompts/system_prompt.py` - Informacja dla LLM
- System prompt informuje asystenta o istnieniu bazy wiedzy
- LLM wie, Å¼e moÅ¼e odpowiadaÄ‡ na pytania o system/dokumentacjÄ™

---

## ğŸ¯ PrzykÅ‚ady uÅ¼ycia

### Pytania, ktÃ³re uruchomiÄ… RAG:

âœ… "Jak dziaÅ‚a aplikacja ParagonyV2?"
âœ… "Jakie sÄ… gÅ‚Ã³wne funkcjonalnoÅ›ci systemu?"
âœ… "WyjaÅ›nij architekturÄ™ aplikacji"
âœ… "Co to jest API asystenta?"
âœ… "Jakie technologie zostaÅ‚y uÅ¼yte?"
âœ… "Pomoc - jak korzystaÄ‡ z aplikacji?"

### Pytania, ktÃ³re NIE uruchomiÄ… RAG:

âŒ "Ile wydaÅ‚em na jedzenie?" (to pytanie o dane uÅ¼ytkownika)
âŒ "Dodaj limit 300 PLN" (to akcja na bazie danych)
âŒ "PokaÅ¼ moje paragony" (to zapytanie o transakcje)

**RAG jest uÅ¼ywany tylko dla pytaÅ„ o dokumentacjÄ™/system, nie o dane uÅ¼ytkownika.**

---

## ğŸ› ï¸ Konfiguracja

### Model embedingowy

**DomyÅ›lny:** `sdadas/mmlw-retrieval-roberta-large`

Dlaczego ten model?
- ğŸ‡µğŸ‡± Stworzony specjalnie dla polskiego jÄ™zyka
- ğŸ¯ Zoptymalizowany dla zadaÅ„ retrieval (wyszukiwania)
- ğŸ† Najlepszy wynik na polskich benchmarkach
- ğŸ“š Idealny dla dokumentacji technicznej

**Alternatywy** (moÅ¼esz zmieniÄ‡ w kodzie):
```python
# W build_rag_database.py i rag_knowledge.py zmieÅ„:
EMBEDDING_MODEL = "sdadas/polish-sentence-transformer"  # lÅ¼ejszy
# lub
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # wielojÄ™zyczny
```

### Parametry chunkingu

W `build_rag_database.py`:

```python
CHUNK_SIZE = 1000       # WielkoÅ›Ä‡ fragmentu w znakach
CHUNK_OVERLAP = 200     # NakÅ‚adanie siÄ™ fragmentÃ³w (dla kontekstu)
```

### Liczba wynikÃ³w RAG

W `rag_knowledge.py`:

```python
def search(self, query: str, top_k: int = 3):  # ZmieÅ„ top_k
```

W `core.py`:

```python
self.rag_kb.get_context_for_query(user_message, max_tokens=500)  # Limit znakÃ³w
```

---

## ğŸ”„ Aktualizacja bazy wiedzy

Gdy dodasz/zmodyfikujesz pliki w `docs/`, uruchom ponownie:

```bash
python build_rag_database.py
```

Baza zostanie przebudowana. Restart aplikacji nie jest wymagany - nowa baza zostanie zaÅ‚adowana przy nastÄ™pnym pytaniu.

---

## ğŸ› Troubleshooting

### âš ï¸ "Baza wiedzy RAG nie istnieje"

```
âš ï¸  Baza wiedzy RAG nie istnieje: backend/assistant_ai/chroma_db
ğŸ’¡ Uruchom: python build_rag_database.py aby jÄ… utworzyÄ‡
```

**RozwiÄ…zanie:** Uruchom `python build_rag_database.py`

### âš ï¸ "Import chromadb could not be resolved"

```
Import "chromadb" could not be resolved
```

**RozwiÄ…zanie:** 
```bash
pip install chromadb sentence-transformers langchain-text-splitters
```

### âš ï¸ Baza siÄ™ nie Å‚aduje mimo Å¼e istnieje

SprawdÅº czy folder `backend/assistant_ai/chroma_db/` zawiera pliki:
```bash
ls backend/assistant_ai/chroma_db/
```

JeÅ›li jest pusty, przebuduj bazÄ™:
```bash
python build_rag_database.py
```

### âš ï¸ Model embedingowy pobiera siÄ™ za dÅ‚ugo

Pierwszy raz model (~500MB) pobiera siÄ™ z HuggingFace. To normalne.

JeÅ›li chcesz lÅ¼ejszy model:
- UÅ¼yj `sdadas/polish-sentence-transformer` (mniejszy)
- Albo `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (najmniejszy)

---

## ğŸ“Š Statystyki i monitoring

### SprawdÅº stan bazy:

```python
from assistant_ai.rag_knowledge import get_rag_knowledge_base

rag = get_rag_knowledge_base()
stats = rag.get_statistics()
print(stats)
```

Output:
```json
{
    "available": true,
    "total_chunks": 127,
    "collection_name": "knowledge_base",
    "embedding_model": "sdadas/mmlw-retrieval-roberta-large",
    "db_path": "backend/assistant_ai/chroma_db"
}
```

### Test wyszukiwania:

```python
results = rag.search("jak dziaÅ‚a API asystenta?", top_k=3)
for i, result in enumerate(results, 1):
    print(f"\n{i}. Å¹rÃ³dÅ‚o: {result['filename']}")
    print(f"   Dopasowanie: {result['distance']:.4f}")
    print(f"   Fragment: {result['text'][:200]}...")
```

---

## ğŸ“ Jak dziaÅ‚a semantyczne wyszukiwanie?

1. **Embedowanie pytania:**
   ```
   "Jak dziaÅ‚a aplikacja?" â†’ [0.234, -0.567, 0.123, ...]
   ```

2. **PorÃ³wnanie z bazÄ…:**
   KaÅ¼dy chunk dokumentacji ma swÃ³j wektor. ChromaDB oblicza **odlegÅ‚oÅ›Ä‡ kosinusowÄ…** miÄ™dzy wektorami.

3. **ZwrÃ³cenie najbliÅ¼szych:**
   Chunki z najmniejszÄ… odlegÅ‚oÅ›ciÄ… (najbardziej podobne semantycznie) sÄ… zwracane.

4. **Dodanie do kontekstu:**
   Te chunki sÄ… formatowane i dodawane do prompta dla LLM.

**To nie jest keyword search!** System rozumie znaczenie, nie tylko sÅ‚owa:
- "jak dziaÅ‚a system" â‰ˆ "wyjaÅ›nij funkcjonalnoÅ›Ä‡ aplikacji"
- "API" â‰ˆ "interfejs programistyczny" â‰ˆ "endpointy"

---

## ğŸ“ Best Practices

### 1. Struktura dokumentacji
- âœ… UÅ¼ywaj nagÅ‚Ã³wkÃ³w (`##`, `###`)
- âœ… Podziel dÅ‚ugie sekcje na mniejsze
- âœ… Dodawaj konkretne przykÅ‚ady
- âœ… UÅ¼ywaj jasnego jÄ™zyka

### 2. WielkoÅ›Ä‡ chunkÃ³w
- 1000 znakÃ³w to sweet spot dla dokumentacji technicznej
- Mniejsze = wiÄ™cej wynikÃ³w ale mniej kontekstu
- WiÄ™ksze = mniej wynikÃ³w ale peÅ‚niejszy kontekst

### 3. Aktualizacja bazy
- Przebuduj bazÄ™ po kaÅ¼dej wiÄ™kszej zmianie w docs/
- MaÅ‚e poprawki = przebuduj wieczorem/w nocy
- DuÅ¼e zmiany = przebuduj od razu

### 4. Monitorowanie
- Sprawdzaj logi czy RAG jest uÅ¼ywany
- Testuj czy odpowiedzi zawierajÄ… informacje z dokumentacji
- Analizuj ktÃ³re pytania uruchamiajÄ… RAG

---

## ğŸ”® PrzyszÅ‚e usprawnienia

Potencjalne rozszerzenia systemu:

- [ ] **Hybrid search** - poÅ‚Ä…czenie semantic + keyword search
- [ ] **Reranking** - drugie sortowanie wynikÃ³w dla lepszej precyzji
- [ ] **Metadata filtering** - filtrowanie po typie dokumentu, dacie itp.
- [ ] **Query expansion** - automatyczne rozszerzanie zapytaÅ„
- [ ] **Cache** - cachowanie popularnych pytaÅ„
- [ ] **Analytics** - statystyki uÅ¼ycia RAG

---

## ğŸ“š Dodatkowe zasoby

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [Model sdadas/mmlw-retrieval-roberta-large](https://huggingface.co/sdadas/mmlw-retrieval-roberta-large)
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)

---

## ğŸ‘¥ Wsparcie

JeÅ›li masz pytania lub problemy:
1. SprawdÅº sekcjÄ™ **Troubleshooting** powyÅ¼ej
2. Przejrzyj kod - jest dobrze udokumentowany
3. SprawdÅº logi w terminalu
4. Zweryfikuj czy wszystkie zaleÅ¼noÅ›ci sÄ… zainstalowane

**Happy coding! ğŸš€**
