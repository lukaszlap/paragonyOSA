# -*- coding: utf-8 -*-
"""
Skrypt do budowy bazy wiedzy RAG z dokumentacji
===================================================

Ten skrypt NIE jest czÄ™Å›ciÄ… gÅ‚Ã³wnego systemu.
SÅ‚uÅ¼y tylko do jednorazowego utworzenia bazy wektorowej z dokumentacji.

UÅ¼ycie:
-------
1. Upewnij siÄ™ Å¼e masz zainstalowane zaleÅ¼noÅ›ci:
   pip install sentence-transformers chromadb langchain-text-splitters

2. Uruchom skrypt:
   python build_rag_database.py

3. Skrypt utworzy plik 'knowledge_base.db' w folderze backend/assistant_ai/

Model embedingowy:
------------------
UÅ¼ywamy 'sdadas/mmlw-retrieval-roberta-large' - najlepszy model dla polskiego jÄ™zyka
w zadaniach retrieval, stworzony przez polskiego naukowca.

Alternatywy:
- sdadas/polish-sentence-transformer
- sentence-transformers/paraphrase-multilingual-mpnet-base-v2

WiÄ™cej info: https://huggingface.co/sdadas/mmlw-retrieval-roberta-large
"""

import sys
from pathlib import Path
from typing import List, Dict
import chromadb
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib


class RAGDatabaseBuilder:
    """Klasa do budowy bazy wiedzy RAG z dokumentacji markdown"""
    
    # Model embedingowy zoptymalizowany dla polskiego jÄ™zyka
    EMBEDDING_MODEL = "sdadas/mmlw-retrieval-roberta-large"
    
    # WielkoÅ›Ä‡ chunkÃ³w - zoptymalizowana dla dokumentacji technicznej
    CHUNK_SIZE = 1000  # znaki
    CHUNK_OVERLAP = 200  # nakÅ‚adanie siÄ™ chunkÃ³w dla kontekstu
    
    def __init__(self, docs_folder: str, output_db_path: str):
        """
        Inicjalizacja buildera
        
        Args:
            docs_folder: ÅšcieÅ¼ka do folderu z dokumentacjÄ… (markdown)
            output_db_path: ÅšcieÅ¼ka gdzie zapisaÄ‡ bazÄ™ .db
        """
        self.docs_folder = Path(docs_folder)
        self.output_db_path = Path(output_db_path)
        
        print("ğŸ”§ Inicjalizacja RAG Database Builder...")
        print(f"ğŸ“ Folder dokumentacji: {self.docs_folder}")
        print(f"ğŸ’¾ Baza zostanie zapisana: {self.output_db_path}")
        
        # Inicjalizacja modelu embedingowego
        print(f"\nğŸ¤– Åadowanie modelu embedingowego: {self.EMBEDDING_MODEL}")
        print("â³ To moÅ¼e potrwaÄ‡ chwilÄ™ przy pierwszym uruchomieniu...")
        self.embedding_model = SentenceTransformer(self.EMBEDDING_MODEL)
        print("âœ… Model zaÅ‚adowany!")
        
        # Inicjalizacja text splittera
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.CHUNK_SIZE,
            chunk_overlap=self.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]  # Preferuj naturalne podziaÅ‚y
        )
        
        # Inicjalizacja ChromaDB
        self.client = chromadb.PersistentClient(
            path=str(self.output_db_path.parent / "chroma_db")
        )
        
    def load_markdown_files(self) -> List[Dict[str, str]]:
        """
        Åaduje wszystkie pliki .md z folderu docs
        
        Returns:
            Lista sÅ‚ownikÃ³w {filename, content, path}
        """
        print("\nğŸ“š Wczytywanie plikÃ³w markdown...")
        documents = []
        
        for md_file in self.docs_folder.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                documents.append({
                    'filename': md_file.name,
                    'content': content,
                    'path': str(md_file)
                })
                print(f"  âœ“ {md_file.name} ({len(content)} znakÃ³w)")
                
            except Exception as e:
                print(f"  âœ— BÅ‚Ä…d przy wczytywaniu {md_file.name}: {e}")
        
        print(f"\nâœ… Wczytano {len(documents)} plikÃ³w")
        return documents
    
    def split_documents(self, documents: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Dzieli dokumenty na mniejsze chunki
        
        Args:
            documents: Lista dokumentÃ³w
            
        Returns:
            Lista chunkÃ³w z metadanymi
        """
        print("\nâœ‚ï¸  Dzielenie dokumentÃ³w na chunki...")
        all_chunks = []
        
        for doc in documents:
            # Podziel tekst na chunki
            text_chunks = self.text_splitter.split_text(doc['content'])
            
            # Dodaj metadane do kaÅ¼dego chunka
            for i, chunk in enumerate(text_chunks):
                chunk_id = hashlib.md5(
                    f"{doc['filename']}_{i}_{chunk[:50]}".encode()
                ).hexdigest()
                
                all_chunks.append({
                    'id': chunk_id,
                    'text': chunk,
                    'filename': doc['filename'],
                    'chunk_index': i,
                    'total_chunks': len(text_chunks),
                    'source_path': doc['path']
                })
            
            print(f"  âœ“ {doc['filename']}: {len(text_chunks)} chunkÃ³w")
        
        print(f"\nâœ… Utworzono {len(all_chunks)} chunkÃ³w")
        return all_chunks
    
    def create_embeddings(self, chunks: List[Dict[str, str]]) -> List[List[float]]:
        """
        Tworzy embedingi dla wszystkich chunkÃ³w
        
        Args:
            chunks: Lista chunkÃ³w
            
        Returns:
            Lista wektorÃ³w embedingowych
        """
        print("\nğŸ§® Generowanie embedingÃ³w...")
        print("â³ To moÅ¼e potrwaÄ‡ kilka minut w zaleÅ¼noÅ›ci od iloÅ›ci danych...")
        
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            batch_size=32
        )
        
        print(f"âœ… Wygenerowano {len(embeddings)} embedingÃ³w")
        print(f"ğŸ“Š Wymiar wektora: {len(embeddings[0])}")
        
        return embeddings.tolist()
    
    def save_to_chromadb(self, chunks: List[Dict[str, str]], embeddings: List[List[float]]):
        """
        Zapisuje chunki i embedingi do ChromaDB
        
        Args:
            chunks: Lista chunkÃ³w z metadanymi
            embeddings: Lista embedingÃ³w
        """
        print("\nğŸ’¾ Zapisywanie do bazy ChromaDB...")
        
        # UsuÅ„ starÄ… kolekcjÄ™ jeÅ›li istnieje
        try:
            self.client.delete_collection("knowledge_base")
            print("  â„¹ï¸  UsuniÄ™to starÄ… kolekcjÄ™")
        except Exception:
            pass
        
        # UtwÃ³rz nowÄ… kolekcjÄ™
        collection = self.client.create_collection(
            name="knowledge_base",
            metadata={
                "description": "Baza wiedzy z dokumentacji ParagonyV2",
                "embedding_model": self.EMBEDDING_MODEL
            }
        )
        
        # Przygotuj dane do zapisu
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [
            {
                'filename': chunk['filename'],
                'chunk_index': chunk['chunk_index'],
                'total_chunks': chunk['total_chunks'],
                'source_path': chunk['source_path']
            }
            for chunk in chunks
        ]
        
        # Zapisz w batch'ach (ChromaDB ma limity)
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            batch_end = min(i + batch_size, len(ids))
            
            collection.add(
                ids=ids[i:batch_end],
                embeddings=embeddings[i:batch_end],
                documents=documents[i:batch_end],
                metadatas=metadatas[i:batch_end]
            )
            
            print(f"  âœ“ Zapisano batch {i//batch_size + 1}/{(len(ids)-1)//batch_size + 1}")
        
        print(f"\nâœ… Baza zapisana w: {self.output_db_path.parent / 'chroma_db'}")
        print(f"ğŸ“Š Liczba chunkÃ³w w bazie: {collection.count()}")
    
    def build(self):
        """GÅ‚Ã³wna funkcja budujÄ…ca bazÄ™ wiedzy"""
        print("\n" + "="*60)
        print("ğŸš€ START BUDOWY BAZY WIEDZY RAG")
        print("="*60)
        
        try:
            # 1. Wczytaj pliki markdown
            documents = self.load_markdown_files()
            
            if not documents:
                print("âŒ Nie znaleziono Å¼adnych plikÃ³w markdown!")
                return False
            
            # 2. Podziel na chunki
            chunks = self.split_documents(documents)
            
            # 3. Wygeneruj embedingi
            embeddings = self.create_embeddings(chunks)
            
            # 4. Zapisz do ChromaDB
            self.save_to_chromadb(chunks, embeddings)
            
            print("\n" + "="*60)
            print("âœ… BAZA WIEDZY RAG UTWORZONA POMYÅšLNIE!")
            print("="*60)
            print("\nğŸ“ NastÄ™pne kroki:")
            print("1. Baza jest gotowa do uÅ¼ycia")
            print("2. System asystenta automatycznie jÄ… zaÅ‚aduje")
            print("3. MoÅ¼esz teraz uÅ¼ywaÄ‡ asystenta z bazÄ… wiedzy")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ BÅÄ„D podczas budowy bazy: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """GÅ‚Ã³wna funkcja skryptu"""
    # ÅšcieÅ¼ki
    current_dir = Path(__file__).parent
    docs_folder = current_dir.parent / "docs"
    output_db = current_dir / "assistant_ai" / "knowledge_base.db"
    
    # SprawdÅº czy folder docs istnieje
    if not docs_folder.exists():
        print(f"âŒ Folder dokumentacji nie istnieje: {docs_folder}")
        print("ğŸ’¡ Upewnij siÄ™ Å¼e uruchamiasz skrypt z folderu 'backend'")
        sys.exit(1)
    
    # UtwÃ³rz folder na bazÄ™ jeÅ›li nie istnieje
    output_db.parent.mkdir(parents=True, exist_ok=True)
    
    # Buduj bazÄ™
    builder = RAGDatabaseBuilder(
        docs_folder=str(docs_folder),
        output_db_path=str(output_db)
    )
    
    success = builder.build()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
